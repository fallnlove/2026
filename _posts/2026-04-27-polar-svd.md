---
layout: distill
title: 'Faster SVD via Accelerated Newton-Schulz Iteration'
description: Traditional SVD algorithms rely heavily on QR factorizations, which scale poorly on GPUs. We show how the recently proposed Chebyshev-Accelerated Newton-Schulz (CANS) iteration can replace them and produce an SVD routine that is faster across a range of matrix types and precisions.
date: 2026-04-27
future: true
htmlwidgets: true
hidden: true

mermaid:
  enabled: true
  zoomable: true

authors:
  - name: Anonymous

bibliography: 2026-04-27-polar-svd.bib

toc:
  - name: Algorithm
  - name: Experiments
    subsections:
      - name: Algorithm Accuracy
      - name: Low-Rank Matrices Analyzes
  - name: SVD via Polar Decomposition
  - name: How to Compute Polar Decomposition?
    subsections:
      - name: Rational Functions (QDWH)
      - name: Polynomial Functions (Newton-Schulz Iteration)
      - name: Accelerated Newton-Schulz Iteration
  - name: Why Matrix Multiplications Instead of QR?
  - name: Detailed Algorithm Description
  - name: Related Work
    subsections:
      - name: CUDA QR (gesvd)
      - name: CUDA Jacobi (gesvdj)
      - name: CUDA Polar (gesvdp)
  - name: Discussion
  - name: Appendix

_styles: >
  .def {
    background: #fcf5fb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
    border-radius: 8px; /* rounded corners */
    padding: 8px 12px; /* small inner spacing from edges */
  }
  .def p {
    color: white;
    text-align: left;
    margin: 12px 0;
    font-size: 16px;
  }
  html[data-theme="dark"] .def {
    background: #404040; /* dark variant */
    border: 1px solid rgba(255, 255, 255, 0.25);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.8);
  }
  html[data-theme="dark"] .def p {
    color: #f5ebff; /* readable light text */
  }
  .small-fig {
    width: 70%;
    height: auto;
    margin-left: auto;
    margin-right: auto;
    display: block;
  }
  .medium-fig {
    width: 80%;
    height: auto;
    margin-left: auto;
    margin-right: auto;
    display: block;
  }
  .def summary {
    cursor: pointer;
    padding: 6px 12px;
    font-weight: 600;
    list-style: none;
  }
  .def summary::-webkit-details-marker { /* keep default triangle but slightly larger */
    font-size: 1.1em;
  }
  .def[open] {
    /* slightly stronger shadow when open */
    box-shadow: 0 2px 8px rgba(0,0,0,0.12);
  }
---

In recent years, the polar decomposition has attracted considerable attention, with the Muon optimizer <d-cite key="jordan2024muon"></d-cite> making a major contribution to its renewed popularity.
Riding this wave of interest, several accelerated methods for computing the polar decomposition have emerged: CANS by Grishina et al. <d-cite key="grishina2025accelerating"></d-cite> and Polar-Express by Amsel et al. <d-cite key="amsel2025polar"></d-cite>, which solely rely on matrix multiplications. Combined with advances in GPU-optimized matrix-multiplication kernels and the introduction of TF-32 and BF-16 precisions, these developments have made polar decomposition remarkably efficient on modern hardware.

In this blogpost, we show that these ideas can be used to accelerate the computation of the singular value decomposition (SVD) via the polar decomposition on GPUs, yielding speedups of up to 2× compared with existing implementations.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/square.webp" class="img-fluid rounded z-depth-2 only-light medium-fig" %}
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/square_dark.webp" class="img-fluid rounded z-depth-2 only-dark medium-fig" %}
    </div>
</div>
<div class="caption">
Algorithm comparison on a random square matrix on an NVIDIA B200 GPU.
<strong>Note:</strong> the algorithms have different accuracy, see Tables 1 and 2 for a more comprehensive analysis.
</div>

Because the CANS iteration relies heavily on matrix multiplications, it is advantageous to use the lower-precision TensorFloat-32 format to speed up computation.
Consequently, CANS-SVD can be executed in two modes:
(1) full FP-32 precision for a more accurate decomposition, or
(2) TF-32 precision for a faster but less accurate result.
Other SVD implementations typically do not offer this level of precision flexibility.

## Algorithm

Our method builds on the approach of computing the SVD via polar decomposition proposed in <d-cite key="nakatsukasa2013stable"></d-cite>,
but replaces the original polar factor computation with the modern techniques of <d-cite key="grishina2025accelerating"></d-cite>. The resulting algorithm can be implemented in just a few lines of code:

```python
def cans_svd(A: matrix):
    _A = cans_preprocessing(A)     # preproccess matrix A
    W = ns_base_iteration(_A)      # compute the polar factor of A
    H = W.T @ A                    # obtain the H - symmetric matrix
    V, S = eigh(H)                 # eigendecomposition of symmetric matrix
    U = W @ V
    U, _ = qr(U)                   # compute QR of U to fix orthogonality
    return U, S, V.T
```

{% details What is SVD? %}
For a matrix $A \in \mathbb{R}^{m \times n}$, the singular value decomposition defined as

$$
A = U \Sigma V^{\top},
$$

where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m \times n}$ is a diagonal matrix whose diagonal entries are the non-negative singular values $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_{\min(m, n)} \ge 0$.
{% enddetails %}

{% details What is polar decomposition? %}
Any matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ admits a polar decomposition

$$
A = WH,
$$

where $W \in \mathbb{R}^{m \times n}$ has orthonormal columns and $H \in \mathbb{R}^{n \times n}$ is symmetric positive semidefinite.
The polar factor $W$ can be viewed as the closest matrix with orthonormal columns to $A$ in the Frobenius norm.
{% enddetails %}

Here, `cans_preprocessing` refers to preprocessing method introduced in <d-cite key="grishina2025accelerating"></d-cite>.
And `ns_base_iteration` refers to iteration based on Newton-Schulz method, which efficiently compute the polar factor <d-cite key="grishina2025accelerating,amsel2025polar,chen2014stable"></d-cite>. Both steps can be executed in TF-32. Often, the bottleneck in computing the SVD via polar decomposition is the computation of the polar factor. Indeed the `eigh` and `qr` steps typically are more fast, therefore, we focus on accelerating the polar factor computation.
A ready-to-use JAX implementation can be found [here](https://github.com/anonymous-502475/polar-svd).

## Experiments

We compare our algorithm with standard GPU-based SVD implementations from NVIDIA's [cuSOLVER](https://docs.nvidia.com/cuda/cusolver/index.html) library and observe the following results:

- On the NVIDIA B200 GPU, the TF-32 variant of CANS-SVD achieves up to a 2× speedup over the CUDA Polar-based SVD and up to a 10× speedup over the CUDA QR- and Jacobi-based SVD.
- The CUDA Polar-based SVD implementation (`driver=svdp`) exhibits numerical instability on ill-conditioned or singular matrices, limiting its practical applicability.
- The ability to use TF-32 precision is a unique advantage of CANS-SVD, whereas cuSOLVER algorithms rely on more complex procedures (such as QR and Givens rotations) that can only be computed in FP-32.
- The FP-32 variant of CANS-SVD consistently provides high accuracy across all test scenarios, while the TF-32 variant maintains a similar orthogonality error but exhibits higher reconstruction error. This gives rise to a trade-off between accuracy and precision.

### Algorithm Accuracy

To assess numerical stability, we generate random matrices $A \in \mathbb{R}^{4096 \times 4096}$ with varying condition numbers $c = ||A||_2 ||A^{-1}||_2$, where $||\cdot||_2$ is the spectral norm. The condition number is a key factor influencing stability and accuracy of numerical linear algebra algorithms <d-cite key="higham2002accuracy"></d-cite>.
We report relative reconstruction and orthogonality errors:

$$\texttt{err}_{\texttt{rec}} = \frac{|| A - U \Sigma V^\top ||_F}{||A||_F}, \quad \texttt{err}_{\texttt{ort}} = \max\left\{\frac{||U^\top U - I||_F}{||I||_F}, \frac{||V^\top V - I||_F}{||I||_F}\right\}.$$


#### Table 1. Relative <u>Reconstruction error</u> for different condition number values.

| Method           | $c=1.1$           | $c=10$            | $c=10^2$          | $c=10^4$          |
| ---------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| CUDA POLAR       | $5.4\cdot10^{-6}$ | $4.3\cdot10^{-6}$ | $5.4\cdot10^{-6}$ | <span style="color:red"> $7.6\cdot10$</span>      |
| CUDA QR          | $1.8\cdot10^{-5}$ | $1.6\cdot10^{-5}$ | $2.7\cdot10^{-5}$ | $1.7\cdot10^{-5}$ |
| CUDA JACOBI      | $1.9\cdot10^{-3}$ | $9.8\cdot10^{-4}$ | $8.4\cdot10^{-4}$ | $6.0\cdot10^{-4}$ |
| CANS-SVD (TF-32) | $6.5\cdot10^{-4}$ | $8.6\cdot10^{-4}$ | $7.1\cdot10^{-4}$ | $7.8\cdot10^{-4}$ |
| CANS-SVD (FP-32) | $3.9\cdot10^{-6}$ | $4.7\cdot10^{-6}$ | $4.1\cdot10^{-6}$ | $4.4\cdot10^{-6}$ |

#### Table 2. Relative <u>Orthogonality error</u> for different condition number values.

| Method           | $c=1.1$           | $c=10$            | $c=10^2$          | $c=10^4$          |
| ---------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| CUDA POLAR       | $5.5\cdot10^{-6}$ | $4.1\cdot10^{-6}$ | $4.0\cdot10^{-6}$ | $8.1\cdot10^{-6}$ |
| CUDA QR          | $1.4\cdot10^{-5}$ | $8.9\cdot10^{-6}$ | $1.3\cdot10^{-5}$ | $1.4\cdot10^{-5}$ |
| CUDA JACOBI      | $2.9\cdot10^{-3}$ | $2.1\cdot10^{-3}$ | $2.1\cdot10^{-3}$ | $2.3\cdot10^{-3}$ |
| CANS-SVD (TF-32) | $3.1\cdot10^{-6}$ | $3.1\cdot10^{-6}$ | $3.0\cdot10^{-6}$ | $2.7\cdot10^{-6}$ |
| CANS-SVD (FP-32) | $3.1\cdot10^{-6}$ | $3.1\cdot10^{-6}$ | $3.0\cdot10^{-6}$ | $2.7\cdot10^{-6}$ |

In the table, values highlighted in <span style="color:red">red</span> correspond to cases where the algorithm failed to compute the SVD.

### Low-Rank Matrices Analyzes

A substantial part of our evaluation focuses on the behavior of the algorithms on low-rank matrices.
For iterative schemes, convergence can be significantly harder to achieve when the input matrix is not full rank.
To illustrate this phenomenon, the figure below compares performance on randomly generated low-rank matrices.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/low_rank.webp" class="img-fluid rounded z-depth-1 only-light medium-fig" %}
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/low_rank_dark.webp" class="img-fluid rounded z-depth-1 only-dark medium-fig" %}
    </div>
</div>
<div class="caption">
  Algorithm comparison on a random square low-rank matrix on NVIDIA B200 GPU (rank is 10× smaller than the matrix size). We do not report CUDA POLAR algorithm in this figure, as the method failed to compute the SVD. See Table 3 below for details.
</div>

Since orthogonality error remains relatively stable across ranks, we focus on reconstruction error for random matrices $A \in \mathbb{R}^{4096 \times 4096}$ with varying rank.

#### Table 3. Relative <u>Reconstruction error</u> for different ranks.

| Method           | $r=16$           | $r=256$            | $r=4095$          | $r=4096$          |
| ---------------- | ----------------- | ----------------- | ----------------- | ----------------- |
| CUDA POLAR       | <span style="color:red"> $7.2 \cdot10$</span> | <span style="color:red"> $7.6\cdot10$</span> | <span style="color:red"> $7.6\cdot10$</span> | $2.4\cdot10^{-6}$ |
| CUDA QR          | $4.3\cdot10^{-7}$ | $8.7\cdot10^{-7}$ | $1.7\cdot10^{-5}$ | $1.6\cdot10^{-5}$ |
| CUDA JACOBI      | $6.9\cdot10^{-5}$ | $3.8\cdot10^{-4}$ | $2.6\cdot10^{-3}$ | $2.6\cdot10^{-3}$ |
| CANS-SVD (TF-32) | $5.1\cdot10^{-4}$ | $6.4\cdot10^{-4}$ | $8.5\cdot10^{-4}$ | $7.6\cdot10^{-4}$ |
| CANS-SVD (FP-32) | $4.1\cdot10^{-5}$ | $3.4\cdot10^{-6}$ | $8.5\cdot10^{-6}$ | $2.8\cdot10^{-6}$ |

As shown above, the CUDA POLAR algorithm failed to compute the SVD even when the rank of the matrix was at least one less than its size.

## SVD via Polar Decomposition

Let us discuss in detail how to compute the SVD using the polar decomposition.
There is a fundamental relation that connects these two decompositions:

$$
A
= \underbrace{U V}_{W}{}^\top
\;\underbrace{V \Sigma V}_{H}{}^\top,
$$

where $A = U \Sigma V^\top$ is the SVD of the matrix $A$.
It is straightforward to check that $W$ has orthonormal columns and that $H$ is symmetric positive semidefinite.
Thus, given the SVD, one can easily obtain the corresponding polar decomposition.

The main idea of computing the SVD via the polar decomposition is to first compute the polar factor $W$ using an iterative method and then compute the eigenvalue decomposition of

$$
H = W^\top A.
$$

Since $H$ is symmetric, its eigendecomposition can be computed using well-established algorithms <d-cite key="nakatsukasa2013stable"></d-cite>.
This approach was developed and further discussed by Nakatsukasa and Higham in <d-cite key="nakatsukasa2013stable,higham2015faster"></d-cite>.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/meme.webp" class="img-fluid rounded z-depth-1 small-fig" %}
    </div>
</div>
<div class="caption">
    Although the polar decomposition is a classical and well-established construct in matrix analysis, and is used in applications such as Muon, it is far less widely known than the SVD or QR decomposition.
</div>

## How to Compute Polar Decomposition?

The practical approach to computing the polar decomposition is to obtain the polar factor using iterative methods.
These iterative methods update a sequence of matrices by transforming their singular values in a controlled manner. At each step, a carefully chosen function $f$ is applied to the singular values of the current iterate, and repeated composition of $f$ is designed to drive all singular values toward $1$.

$$
X_{k + 1} = f(X_k) = f(U \Sigma_k V^\top) = U f(\Sigma_k) V^\top, \quad X_0 = U \Sigma_0 V^\top
$$

Typically, the function $f$ is chosen from polynomials or rational functions. It is crucial to select $f$ so that it acts on a matrix through its singular values, admitting the relation $f(U \Sigma V^\top) = U f(\Sigma) V^\top$.
Once all singular values approach $1$, the iterate converges to the product of the left and right singular vectors,
which yields the [polar factor](#svd-via-polar-decomposition):

$$
X_k \to U V^\top = W, \quad \text{when } f(\Sigma_k) \to I.
$$

The choice of $f$ strongly influences convergence speed: the rate at which the iterated composition
$f(f(\dots f(x)))$ drives singular values toward $1$ determines how quickly the method approaches the polar factor. Below, we investigate rational functions (QDWH) and polynomial functions (Newton-Schulz iteration).

### Rational Functions (QDWH)

There are several iterative methods that are based on rational functions available, including Newton iteration <d-cite key="kenney1992scaling"></d-cite> and Halley's <d-cite key="gander1985halley,gander1990algorithms"></d-cite> iteration:

$$
X_{k + 1} = X_k(3I + X_k^\top X_k) (I + 3X_k^\top X_k)^{-1}, \quad X_0 = A.
$$

However, these methods are not practical because they require matrix inversion. To address this, in 2010 Nakatsukasa et al. <d-cite key="nakatsukasa2010optimizing"></d-cite> introduced the QR-based dynamically weighted Halley iteration (QDWH), demonstrating that Halley's iteration can be implemented without matrix inversion using QR decomposition. Existing stable methods for computing QR decomposition make QDWH preferable for
practical usage. Moreover, QDWH algorithm is used in the CUDA implementation of Polar-based SVD <d-cite key="nakatsukasa2013stable"></d-cite>.
Although the QDWH algorithm is highly stable in practice, modern GPU hardware is not optimized for fast QR decomposition. Instead, most optimization effort goes into basic operations such as matrix multiplications.

### Polynomial Functions (Newton-Schulz Iteration)

We next consider the Newton-Schulz iteration <d-cite key="bjorck1971iterative,kovarik1970some"></d-cite>:

$$
X_{k + 1} = \frac{3}{2} X_k - \frac{1}{2} X_k X_k^\top X_k, \quad X_0 = A.
$$

This iteration relies solely on matrix multiplications, which can make it faster in practice than the QDWH algorithm, even though QDWH requires fewer iterations. Newton–Schulz has become increasingly popular in applications, for example, it is used as a core component of the Muon optimizer <d-cite key="jordan2024muon"></d-cite>, where a slightly modified polynomial is employed.

Notably, this method converges if $|| X_0 ||_2 < \sqrt{3}$.
Therefore, the initial matrix should be normalized before the iteration begins.
Ideally, one would divide the matrix by its spectral norm so that $|| X_0 ||_2 = 1$, but computing the spectral norm is expensive.
In our experiments, we use a normalization method based on the $1$-norm and $\infty$-norm from QDWH implementation in [JAX](https://github.com/jax-ml/jax/blob/23db456a8acd01a04ed5a9f87f8265cc21703926/jax/_src/tpu/linalg/qdwh.py#L132):

$$
X_0 = A / \sqrt{\| A \|_1 \| A \|_{\infty}}, \quad \text{since }
\| A \|_2 \leq \sqrt{\| A \|_1 \| A \|_{\infty}}.
$$

The $1$-norm and $\infty$-norm are straightforward to compute, as they correspond to the maximum $\ell_1$ norm of the columns and rows of the matrix, respectively.

### Accelerated Newton-Schulz Iteration

Recent studies by Grishina et al. <d-cite key="grishina2025accelerating"></d-cite> and Amsel et al. <d-cite key="amsel2025polar"></d-cite> introduce accelerated Newton-Schulz iteration modification based on another polynomial design. These methods named CANS and Polar-Express, respectively.
They provide new optimal strategies to find coefficients for the polynomial.

These methods differ slightly. First, CANS performs preprocessing of the matrix before starting the iteration. Second, in Polar-Express, for numerical stability, all coefficients are divided by $1.01$ except in the last iteration, which affects the accuracy.

write about preprocessing

## Why Matrix Multiplications Instead of QR?

In the previous discussion, we argued that matrix multiplications are far more optimized on modern GPUs than QR decompositions. We now support this claim by comparing the execution time of QR decompositions and matrix multiplications (MM) on various GPUs.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/qr_mm_ratio.webp" class="img-fluid rounded z-depth-2  only-light" %}
        {% include figure.liquid path="assets/img/2026-04-27-polar-svd/qr_mm_ratio_dark.webp" class="img-fluid rounded z-depth-2  only-dark" %}
    </div>
</div>
<div class="caption">
    Comparison illustrating how the number of matrix multiplications that can be executed within the runtime of a single QR decomposition varies with matrix size for a square matrix. Matrix multiplications were performed in TF-32 precision in the left figure and in FP-32 precision in the right figure.
</div>

As shown in the figure above, for medium-sized matrices, matrix multiplication is significantly faster than QR decomposition, with the gap particularly large in TF-32 precision, since most GPU libraries do not support QR in TF-32.
Notably, the gap between the cost of matrix multiplication and QR widens even further on newer GPU architectures, where MM performance improves much more rapidly than QR. Therefore, whenever possible, it is preferable to perform several matrix multiplications rather than a small number of QR decompositions.

## Algorithm Description

In this section, we discuss the [CANS-SVD algorithm](#algorithm) and the details of its implementation.
The procedure begins by preprocessing the input matrix and then computing its polar decomposition using the CANS iteration from <d-cite key="grishina2025accelerating"></d-cite>.
CANS preprocessing is a crucial component of the algorithm, as it significantly accelerates the convergence of the CANS iteration.
Importantly, this acceleration effect persists even when as `ns_base_iteration` is used an alternative iterative method for computing the polar factor (such as Polar-Express).

```python
    _A = cans_preprocessing(A)     # preprocess matrix A
    W = ns_base_iteration(_A)      # compute the polar factor of A
    H = W.T @ A                    # obtain H – the symmetric matrix
```

After obtaining the polar factor, we compute the symmetric eigendecomposition of the matrix $H$ using standard algorithms available in numerical linear algebra libraries, such as those provided in JAX.

```python
    V, S = eigh(H) # eigendecomposition of symmetric matrix
    U = W @ V
```

Matrix $A$ can then be decomposed as `A = U @ diag(S) @ V.T`.
However, if $A$ is singular, both QDWH- and NS-based methods converge to a singular matrix $W$, and the resulting matrix $U$ will not be orthogonal.

To address this issue, existing CUDA implementations employ certain engineering workarounds, adding a small perturbation to matrices, as described in the [cuSOLVER documentation](https://docs.nvidia.com/cuda/cusolver/index.html#cusolverdnxgesvdp).
Since such perturbations may affect the accuracy of the singular values, we do not use them. Instead, following <d-cite key="nakatsukasa2013stable"></d-cite>, we perform a QR decomposition of the matrix of left singular vectors to restore orthogonality, which provides an effective solution for handling singular matrices:

```python
    U, _ = qr(U)  # compute QR of U to fix orthogonality
```

## Related Work

### CUDA QR (`gesvd`)

QR-based algorithms reduce a matrix to bidiagonal form via Householder reflections (or Givens rotations) and then iteratively apply implicit QR steps to compute singular values. These methods are robust and numerically stable, forming the backbone of LAPACK's SVD routines <d-cite key="demmel1990accurate"></d-cite>.

However, despite their robustness, QR-based SVD algorithms are poorly parallelizable: Householder (or Givens) bidiagonalization involves long dependency chains. This makes them GPU-unfriendly and significantly less efficient on modern massively parallel architectures <d-cite key="struski2024efficient"></d-cite>.


### CUDA Jacobi (`gesvdj`)

Jacobi-based SVD applies a sequence of plane rotations (Givens rotations) to eliminate the off-diagonal entries of $A^\top A$. At each step, the algorithm picks a pair of columns, computes a $2 \times 2$ rotation that makes them orthogonal, and updates the matrix. Repeating these pairwise orthogonalizations drives the matrix toward a diagonal form.

Because many independent column pairs can be processed simultaneously, the method is highly parallel and well suited for GPUs. The parallelism of Jacobi method gives GPU better performance on small and medium size matrices than QR-based method ([cuSOLVER](https://docs.nvidia.com/cuda/cusolver/index.html)).

### CUDA Polar (`gesvdp`)

## Discussion

The blog post shows how an SVD algorithm based on polar decomposition can be accelerated by replacing the QDWH method (which relies on QR factorizations) with the CANS iteration (which uses only matrix multiplications). However, we do not address the symmetric eigenvalue decomposition, which is also traditionally computed via a sequence of QR iterations and which we have not discussed above. It is therefore likely that this task could also be accelerated by replacing those iterations with faster matrix-multiplication–based methods. We leave this as a direction for future research.

## Appendix

For the first part of our experiments, we consider square random matrices $A \in \mathbb{R}^{n \times n}$ with condition number $c = 10$.
Formally, we generate two random matrices $G_1$ and $G_2$ with i.i.d. entries drawn from the standard Gaussian distribution.
We then obtain matrices $U$ and $V$ as the Q factors from the QR decompositions of $G_1$ and $G_2$, respectively, and construct

$$
A = U \Sigma V^\top,
$$

where $\Sigma$ is a diagonal matrix with entries $\sigma_i = c^{(n - i) / (n - 1)}$.
This procedure allows us to generate random matrices with prescribed condition numbers.

We generate random matrices with a fixed rank $r$ using a similar scheme, except that

$$
\sigma_i =
\begin{cases}
1, & i \le r, \\
0, & \text{otherwise},
\end{cases}
$$

so that the resulting matrix has exactly rank $r$.

For the CANS algorithm <d-cite key="grishina2025accelerating"></d-cite>, we use the following
hyperparameters:

|                    | TF-32 version | FP-32 version |
| ------------------ | ------------- | ------------- |
| `degree`           | $3$           | $3$           |
| `preprocess_iters` | $2$           | $2$           |
| `delta`            | $0.99$        | $0.99$        |
| `max_iter`         | $50$          | $50$          |
| `tolerance`        | $10^{-3}$     | $10^{-5}$     |
